{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOk4y3yNQS+UZWXHg8fl+7Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PuchToTalk/Football_market-value/blob/Airflow-dag/my_dag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 & 2 : DAG structure to load our 2 CSV"
      ],
      "metadata": {
        "id": "dTRxts3uWtG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 1 :"
      ],
      "metadata": {
        "id": "452i6JzmLatB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DieKTJYSdEMT"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "with DAG(\n",
        "       'my_first_dag',\n",
        "       default_args={\n",
        "           'depends_on_past': False,\n",
        "           'email': ['airflow@example.com'],\n",
        "           'email_on_failure': False,\n",
        "           'email_on_retry': False,\n",
        "           'retries': 1,\n",
        "           'retry_delay': timedelta(minutes=5),\n",
        "       },\n",
        "       description='A first DAG',\n",
        "       schedule=None,\n",
        "       start_date=datetime(2023, 1, 1),\n",
        "       catchup=False,\n",
        "       tags=['example'],\n",
        "\n",
        ") as dag:\n",
        "   dag.doc_md = \"\"\"\n",
        "       This is my DAG in airflow for the Big Data Project.\n",
        "       I can write documentation in Markdown here with **bold text** or __bold text__.\n",
        "   \"\"\"\n",
        "\n",
        "   import pandas as pd\n",
        "   import pyarrow as pa\n",
        "   import pyarrow.parquet as pq\n",
        "\n",
        "\n",
        "   def load_foot_perf():\n",
        "       print(\"load foot perf task\")\n",
        "       # Chargement du fichier foot_perf.csv\n",
        "       foot_perf_data = pd.read_csv(\"/Users/paulc/airflow/dags/foot_perf.csv\")\n",
        "       #foot_perf_data = foot_perf_data.to_json(orient='records')\n",
        "\n",
        "       # Convert DataFrame to PyArrow Table\n",
        "       foot_perf_data = pa.Table.from_pandas(foot_perf_data)\n",
        "       output_1 = 'output_foot_perf.parquet'\n",
        "       pq.write_table(foot_perf_data, output_1)\n",
        "\n",
        "       # Read Parquet file back into a DataFrame\n",
        "       parquet_table1 = pq.read_table(output_1)\n",
        "       foot_perf_data = parquet_table1.to_pandas()\n",
        "       foot_perf_data = foot_perf_data.to_json(orient='records')\n",
        "       return foot_perf_data\n",
        "\n",
        "   foot_perf_data = load_foot_perf()\n",
        "   #print(foot_perf_data)\n",
        "\n",
        "\n",
        "\n",
        "   def load_market_value():\n",
        "       print(\"load market value task\")\n",
        "       # Chargement du fichier market_value.csv\n",
        "       market_value_data = pd.read_csv(\"/Users/paulc/airflow/dags/only_mv2.csv\")\n",
        "       #market_value_data = market_value_data.to_json(orient='records')\n",
        "       # Convert DataFrame to PyArrow Table\n",
        "       market_value_data = pa.Table.from_pandas(market_value_data)\n",
        "       output_2 = 'output_foot_value.parquet'\n",
        "       pq.write_table(market_value_data, output_2)\n",
        "\n",
        "       # Read Parquet file back into a DataFrame\n",
        "       parquet_table2 = pq.read_table(output_2)\n",
        "       market_value_data = parquet_table2.to_pandas()\n",
        "       market_value_data = market_value_data.to_json(orient='records')\n",
        "       return market_value_data\n",
        "\n",
        "\n",
        "   market_value_data = load_market_value()\n",
        "   #print(market_value_data)\n",
        "\n",
        "\n",
        "   def clean_foot_perf():\n",
        "       print(\"cleaned foot perf data task\")\n",
        "       # Nettoyage et filtrage des données de foot_perf\n",
        "       foot_perf_data = pd.read_json(load_foot_perf())\n",
        "       # Read Parquet file back into a DataFrame\n",
        "       output_1 = 'output_foot_perf.parquet'\n",
        "       # Read Parquet file back into a DataFrame\n",
        "       parquet_table1 = pq.read_table(output_1)\n",
        "       foot_perf_data = parquet_table1.to_pandas()\n",
        "       return foot_perf_data\n",
        "\n",
        "\n",
        "   print(clean_foot_perf())\n",
        "\n",
        "\n",
        "   # Perform further operations with cleaned_foot_perf_data if needed\n",
        "\n",
        "   def clean_market_value(market_value_data):\n",
        "       print(\"formatted market value data task\")\n",
        "       market_value_data = pd.DataFrame(market_value_data)\n",
        "       filtered_data = market_value_data.drop(columns= filtered_data.columns[0], axis=1)\n",
        "       filtered_data = filtered_data.drop([\"Nation\", \"Pos\"], axis=1)\n",
        "       filtered_data = filtered_data.to_dict(orient='records')\n",
        "       return filtered_data\n",
        "\n",
        "\n",
        "   def join_datasets(df_football_stats, market_value_data):\n",
        "       print(\"join 2 datasets task \")\n",
        "       # Jointure des données de foot_perf et market_value\n",
        "       merged_data = df_football_stats.merge(market_value_data, on='player', how='inner')\n",
        "       return merged_data\n",
        "\n",
        "\n",
        "   def index_to_elastic(merged_data):\n",
        "       print(\"merged data task\")\n",
        "       # Indexation des données dans ElasticSearch\n",
        "       # Code pour l'indexation des données dans ElasticSearch\n",
        "       #pass\n",
        "\n",
        "   source_to_raw1 = PythonOperator(\n",
        "        task_id='source_to_raw1',\n",
        "        python_callable=load_foot_perf,\n",
        "        #provide_context=True,  # Add this line to pass the context to the function\n",
        "        #op_kwargs={'foot_perf_data': load_foot_perf()}  # Initialize foot_perf_data with None\n",
        "    )\n",
        "\n",
        "   source_to_raw2 = PythonOperator(\n",
        "       task_id='source_to_raw2',\n",
        "       python_callable=load_market_value,\n",
        "       #provide_context=True,  # Add this line to pass the context to the function\n",
        "       #op_kwargs={'market_value_data': load_market_value()}  # Initialize foot_perf_data with None\n",
        "   )\n",
        "\n",
        "   raw_to_formatted1 = PythonOperator(\n",
        "       task_id='raw_to_formatted1',\n",
        "       python_callable=clean_foot_perf,\n",
        "       #op_kwargs={'foot_perf_data': '{{ ti.xcom_pull(task_ids=\"source_to_raw1\") }}'}\n",
        "\n",
        "   )\n",
        "\n",
        "   raw_to_formatted2 = PythonOperator(\n",
        "        task_id='raw_to_formatted2',\n",
        "        python_callable=clean_market_value,\n",
        "        #op_kwargs={'market_value_data': '{{ ti.xcom_pull(task_ids=\"source_to_raw2\") }}'}\n",
        "\n",
        "    )\n",
        "\n",
        "   produce_usage = PythonOperator(\n",
        "       task_id='produce_usage',\n",
        "       python_callable=join_datasets,\n",
        "\n",
        "   )\n",
        "\n",
        "   index_to_elastic = PythonOperator(\n",
        "       task_id='index_to_elastic',\n",
        "       python_callable=index_to_elastic,\n",
        "\n",
        "   )\n",
        "\n",
        "   source_to_raw1 >> raw_to_formatted1\n",
        "   source_to_raw2 >> raw_to_formatted2\n",
        "   [raw_to_formatted1, raw_to_formatted2] >> produce_usage >> index_to_elastic\n",
        "\n",
        "\n",
        "   if __name__ == 'main':\n",
        "        df = clean_foot_perf(load_foot_perf())\n",
        "        df1 = clean_market_value(load_market_value())\n",
        "        df2 = join_datasets(df, df1)\n",
        "        index_to_elastic(df2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 2 : méthode JSON -> Data -> JSON"
      ],
      "metadata": {
        "id": "7K5nAtRPLYGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.models import DagRun\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "default_args = {\n",
        "    'depends_on_past': False,\n",
        "    'email': ['airflow@example.com'],\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "dag = DAG(\n",
        "    'my_first_dag',\n",
        "    default_args=default_args,\n",
        "    description='A first DAG',\n",
        "    schedule=None,\n",
        "    start_date=datetime(2023, 1, 1),\n",
        "    catchup=False,\n",
        "    tags=['example'],\n",
        ")\n",
        "\n",
        "def load_foot_perf():\n",
        "    print(\"load foot perf task\")\n",
        "    foot_perf_data = pd.read_csv(\"/Users/paulc/airflow/dags/foot_perf.csv\")\n",
        "    print(foot_perf_data)\n",
        "    foot_perf_data_parquet = pa.Table.from_pandas(foot_perf_data)\n",
        "    output_1 = 'output_foot_perf.parquet'\n",
        "    pq.write_table(foot_perf_data_parquet, output_1)\n",
        "    foot_perf_data = foot_perf_data.to_json(orient='records')\n",
        "    print(foot_perf_data)\n",
        "    return foot_perf_datax\n",
        "\n",
        "def load_market_value():\n",
        "    print(\"load market value task\")\n",
        "    market_value_data = pd.read_csv(\"/Users/paulc/airflow/dags/only_mv2.csv\")\n",
        "    market_value_data_parquet = pa.Table.from_pandas(market_value_data)\n",
        "    output_2 = 'output_foot_value.parquet'\n",
        "    pq.write_table(market_value_data_parquet, output_2)\n",
        "    market_value_data = market_value_data.to_json(orient='records')\n",
        "    return market_value_data\n",
        "\n",
        "def clean_foot_perf(ti):\n",
        "    print(\"cleaned foot perf data task\")\n",
        "    foot_perf_data = ti.xcom_pull(task_ids='source_to_raw1')\n",
        "    # Perform cleaning operations on foot_perf_data DataFrame\n",
        "    cleaned_df_perf = pd.read_json(foot_perf_data)\n",
        "    print(cleaned_df_perf)\n",
        "    cleaned_df_perf_data = cleaned_df_perf.to_json(orient='records')\n",
        "    return cleaned_df_perf_data\n",
        "\n",
        "\n",
        "def clean_market_value(ti):\n",
        "    print(\"formatted market value data task\")\n",
        "    market_value_data = ti.xcom_pull(task_ids='source_to_raw2')\n",
        "    # Perform cleaning operations on market_value_data DataFrame\n",
        "    cleaned_df_value = pd.read_json(market_value_data)\n",
        "    print(cleaned_df_value)\n",
        "    cleaned_df_value_data = cleaned_df_value.to_json(orient='records')\n",
        "    return cleaned_df_value_data\n",
        "\n",
        "\n",
        "def join_datasets(ti):\n",
        "    print(\"join 2 datasets task\")\n",
        "    foot_perf_data = ti.xcom_pull(task_ids='raw_to_formatted1')\n",
        "    market_value_data = ti.xcom_pull(task_ids='raw_to_formatted2')\n",
        "    # Join foot_perf_data and market_value_data\n",
        "    merged_data = foot_perf_data.merge(market_value_data, on='player', how='inner')\n",
        "    return merged_data\n",
        "\n",
        "def index_to_elastic(ti):\n",
        "    print(\"index to ElasticSearch task\")\n",
        "    merged_data = ti.xcom_pull(task_ids='combine')\n",
        "    # Index merged_data to ElasticSearch\n",
        "    # ...\n",
        "\n",
        "source_to_raw1 = PythonOperator(\n",
        "    task_id='source_to_raw1',\n",
        "    python_callable=load_foot_perf,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "source_to_raw2 = PythonOperator(\n",
        "    task_id='source_to_raw2',\n",
        "    python_callable=load_market_value,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "raw_to_formatted1 = PythonOperator(\n",
        "    task_id='raw_to_formatted1',\n",
        "    python_callable=clean_foot_perf,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "raw_to_formatted2 = PythonOperator(\n",
        "    task_id='raw_to_formatted2',\n",
        "    python_callable=clean_market_value,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "combine = PythonOperator(\n",
        "    task_id='combine',\n",
        "    python_callable=join_datasets,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "index_to_elastic = PythonOperator(\n",
        "    task_id='index_to_elastic',\n",
        "    python_callable=index_to_elastic,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "source_to_raw1 >> raw_to_formatted1\n",
        "source_to_raw2 >> raw_to_formatted2\n",
        "[raw_to_formatted1, raw_to_formatted2] >> combine >> index_to_elastic\n"
      ],
      "metadata": {
        "id": "8531lL_GLZDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 : ajout du merge"
      ],
      "metadata": {
        "id": "TKZsMykvoni6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.models import DagRun\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from elasticsearch import Elasticsearch, helpers\n",
        "\n",
        "\n",
        "default_args = {\n",
        "    'depends_on_past': False,\n",
        "    'email': ['airflow@example.com'],\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "dag = DAG(\n",
        "    'my_first_dag',\n",
        "    default_args=default_args,\n",
        "    description='A first DAG',\n",
        "    schedule=None,\n",
        "    start_date=datetime(2023, 1, 1),\n",
        "    catchup=False,\n",
        "    tags=['example'],\n",
        ")\n",
        "\n",
        "def load_foot_perf():\n",
        "    print(\"load foot perf task\")\n",
        "    foot_perf_data = pd.read_csv(\"/Users/paulc/airflow/dags/foot_perf.csv\")\n",
        "    print(foot_perf_data)\n",
        "    foot_perf_data_parquet = pa.Table.from_pandas(foot_perf_data)\n",
        "    output_1 = 'output_foot_perf.parquet'\n",
        "    pq.write_table(foot_perf_data_parquet, output_1)\n",
        "    foot_perf_data = foot_perf_data.to_json(orient='records')\n",
        "    print(foot_perf_data)\n",
        "    return foot_perf_data\n",
        "\n",
        "def load_market_value():\n",
        "    print(\"load market value task\")\n",
        "    market_value_data = pd.read_csv(\"/Users/paulc/airflow/dags/only_mv2.csv\")\n",
        "    market_value_data_parquet = pa.Table.from_pandas(market_value_data)\n",
        "    output_2 = 'output_foot_value.parquet'\n",
        "    pq.write_table(market_value_data_parquet, output_2)\n",
        "    market_value_data = market_value_data.to_json(orient='records')\n",
        "    return market_value_data\n",
        "\n",
        "def clean_foot_perf(ti):\n",
        "    print(\"cleaned foot perf data task\")\n",
        "    foot_perf_data = ti.xcom_pull(task_ids='source_to_raw1')\n",
        "    # Perform cleaning operations on foot_perf_data DataFrame\n",
        "    cleaned_df_perf = pd.read_json(foot_perf_data)\n",
        "    print(cleaned_df_perf)\n",
        "    cleaned_df_perf_data = cleaned_df_perf.to_json(orient='records')\n",
        "    return cleaned_df_perf_data\n",
        "\n",
        "\n",
        "def clean_market_value(ti):\n",
        "    print(\"formatted market value data task\")\n",
        "    market_value_data = ti.xcom_pull(task_ids='source_to_raw2')\n",
        "    # Perform cleaning operations on market_value_data DataFrame\n",
        "    cleaned_df_value = pd.read_json(market_value_data)\n",
        "    print(cleaned_df_value)\n",
        "    cleaned_df_value_data = cleaned_df_value.to_json(orient='records')\n",
        "    return cleaned_df_value_data\n",
        "\n",
        "\n",
        "def join_datasets(ti):\n",
        "    print(\"join 2 datasets task\")\n",
        "    cleaned_df_perf = ti.xcom_pull(task_ids='raw_to_formatted1')\n",
        "    cleaned_df_perf = pd.read_json(cleaned_df_perf)\n",
        "    cleaned_df_value = ti.xcom_pull(task_ids='raw_to_formatted2')\n",
        "    cleaned_df_value = pd.read_json(cleaned_df_value)\n",
        "    print(cleaned_df_value)\n",
        "    print(cleaned_df_perf)\n",
        "\n",
        "    # Join foot_perf_data and market_value_data\n",
        "    merged_data = cleaned_df_perf.merge(cleaned_df_value, on='Player', how='inner')\n",
        "    print(merged_data)\n",
        "    merged_data = merged_data.to_json(orient='records')\n",
        "    return merged_data\n",
        "\n",
        "\n",
        "def index_to_elastic(ti):\n",
        "    print(\"Index to Elasticsearch task\")\n",
        "    merged_data = ti.xcom_pull(task_ids='combine')\n",
        "    merged_data_to_elastic = pd.read_json(merged_data)\n",
        "\n",
        "    # Create an Elasticsearch instance\n",
        "    es = Elasticsearch(['http://localhost:9200'])  # Modify the URL if necessary\n",
        "\n",
        "    # Index the data in Elasticsearch\n",
        "    index_name = 'equipe_nation_index'  # Elasticsearch index name\n",
        "    doc_type = 'equipe'  # Elasticsearch document type (deprecated starting from version 7.x)\n",
        "\n",
        "    actions = []\n",
        "    for index, row in merged_data_to_elastic.iterrows():\n",
        "        document = {\n",
        "            \"_index\": index_name,\n",
        "            \"_type\": doc_type,\n",
        "            \"_source\": row.to_dict()\n",
        "        }\n",
        "        actions.append(document)\n",
        "\n",
        "    # Use the \"helpers.bulk\" method to bulk index the documents\n",
        "    response = helpers.bulk(es, actions)\n",
        "\n",
        "    # Check for indexing errors\n",
        "    if response[1]:\n",
        "        print(\"Error indexing data to Elasticsearch.\")\n",
        "        print(response[1])\n",
        "        raise BulkIndexError(\"%i document(s) failed to index.\" % len(response[1]), response[1])\n",
        "    else:\n",
        "        print(\"Data has been successfully indexed to Elasticsearch.\")\n",
        "\n",
        "source_to_raw1 = PythonOperator(\n",
        "    task_id='source_to_raw1',\n",
        "    python_callable=load_foot_perf,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "source_to_raw2 = PythonOperator(\n",
        "    task_id='source_to_raw2',\n",
        "    python_callable=load_market_value,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "raw_to_formatted1 = PythonOperator(\n",
        "    task_id='raw_to_formatted1',\n",
        "    python_callable=clean_foot_perf,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "raw_to_formatted2 = PythonOperator(\n",
        "    task_id='raw_to_formatted2',\n",
        "    python_callable=clean_market_value,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "combine = PythonOperator(\n",
        "    task_id='combine',\n",
        "    python_callable=join_datasets,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "index_to_elastic = PythonOperator(\n",
        "    task_id='index_to_elastic',\n",
        "    python_callable=index_to_elastic,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "source_to_raw1 >> raw_to_formatted1\n",
        "source_to_raw2 >> raw_to_formatted2\n",
        "[raw_to_formatted1, raw_to_formatted2] >> combine >> index_to_elastic\n"
      ],
      "metadata": {
        "id": "fYSH7BL2opLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 : Indexing qui marche"
      ],
      "metadata": {
        "id": "we26UBGDWie7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Elasticsearch instance\n",
        "es_client = connections.create_connection(hosts=['http://localhost:9200/'])\n",
        "\n",
        "test_df = pd.read_csv(\"foot_perf.csv\")\n",
        "\n",
        "def indexing_function(df):\n",
        "\n",
        "    # Index the data in Elasticsearch\n",
        "    df_iter = df.iterrows()\n",
        "    for index, document in df_iter:\n",
        "        yield {\n",
        "            \"_index\": 'equipe_nation_index',\n",
        "            \"_type\": \"_doc\",\n",
        "            \"_source\": document,\n",
        "        }\n",
        "\n",
        "helpers.bulk(es_client, indexing_function(test_df))"
      ],
      "metadata": {
        "id": "DIC3yto7WjfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------"
      ],
      "metadata": {
        "id": "ZzTk49Zd5mwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final DAG"
      ],
      "metadata": {
        "id": "HDryafHf5OI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "my_dag.py"
      ],
      "metadata": {
        "id": "Qcd2P9fX5naA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from elasticsearch import Elasticsearch, helpers\n",
        "from elasticsearch_dsl import connections\n",
        "\n",
        "# Import all other Python Files' functions\n",
        "from index_elastic import indexing_function\n",
        "from formatting_functions import interval_value, interval_age, interval_dribble, interval_goal, interval_match, interval_pass\n",
        "from team_by_nation import filtering_players_by_nation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "default_args = {\n",
        "    'depends_on_past': False,\n",
        "    'email': ['airflow@example.com'],\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "dag = DAG(\n",
        "    'my_first_dag',\n",
        "    default_args=default_args,\n",
        "    description='A first DAG',\n",
        "    schedule=None,\n",
        "    start_date=datetime(2023, 1, 1),\n",
        "    catchup=False,\n",
        "    tags=['example'],\n",
        ")\n",
        "\n",
        "def load_foot_perf():\n",
        "    print(\"Step 1.1 : Load foot perf task\")\n",
        "    foot_perf_data = pd.read_csv(\"/Users/paulc/airflow/dags/CSV/foot_perf.csv\")\n",
        "    print(f\"Our first CSV file raw data : {foot_perf_data}\")\n",
        "\n",
        "    # Creation of the parquet file\n",
        "    foot_perf_data_parquet = pa.Table.from_pandas(foot_perf_data)\n",
        "    output_1 = '/Users/paulc/airflow/dags/Parquet/load_foot_perf.parquet'\n",
        "    pq.write_table(foot_perf_data_parquet, output_1)\n",
        "\n",
        "    # Convert the CSV file to JSON file (because Airflow does not allow Dataframe file type)\n",
        "    foot_perf_data = foot_perf_data.to_json(orient='records')\n",
        "\n",
        "    print(f\"The JSON file : {foot_perf_data}\")\n",
        "    return foot_perf_data\n",
        "\n",
        "#foot_perf_data = load_foot_perf()\n",
        "\n",
        "\n",
        "def load_market_value():\n",
        "    print(\"Step 1.2 : Load market value task\")\n",
        "    market_value_data = pd.read_csv(\"/Users/paulc/airflow/dags/CSV/only_mv2.csv\")\n",
        "    print(f\"Our second CSV file raw data : {market_value_data}\")\n",
        "    # Creation of the parquet file\n",
        "    market_value_data_parquet = pa.Table.from_pandas(market_value_data)\n",
        "    output_2 = '/Users/paulc/airflow/dags/Parquet/load_market_value.parquet'\n",
        "    pq.write_table(market_value_data_parquet, output_2)\n",
        "\n",
        "    # Convert the CSV file to JSON file (for the same reason)\n",
        "    market_value_data = market_value_data.to_json(orient='records')\n",
        "    print(f\"The JSON file : {market_value_data}\")\n",
        "    return market_value_data\n",
        "\n",
        "#market_value_data = load_market_value()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def clean_foot_perf(ti):\n",
        "    print(\"cleaned foot perf data task\")\n",
        "    foot_perf_data = ti.xcom_pull(task_ids='source_to_raw1')\n",
        "    foot_perf_data = pd.read_json(foot_perf_data)\n",
        "    # Formatting our data by filtering on a new dataframe based on the original one\n",
        "    df_football_stats = pd.DataFrame()\n",
        "    df_football_stats['Player'] = foot_perf_data['Player']\n",
        "    df_football_stats['Nation'] = foot_perf_data['Nation']\n",
        "    df_football_stats['Pos'] = foot_perf_data['Pos']\n",
        "    df_football_stats['Squad'] = foot_perf_data['Squad']\n",
        "    df_football_stats['Comp'] = foot_perf_data['Comp']\n",
        "    df_football_stats['Age'] = foot_perf_data['Age']\n",
        "    df_football_stats['MP'] = foot_perf_data['MP']\n",
        "    df_football_stats['G/90'] = foot_perf_data['Goals']\n",
        "    df_football_stats['G/Sh'] = foot_perf_data['G/Sh']\n",
        "    df_football_stats['PKGoals'] = ((foot_perf_data['ShoPK'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['PKAttempted'] = ((foot_perf_data['PKatt'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['Goals'] = ((foot_perf_data['Goals'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['Pass'] = ((foot_perf_data['PasTotAtt'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['PassCompleted'] = ((foot_perf_data['PasTotCmp'] * foot_perf_data['Min']) / 90).round(0).astype(\n",
        "        int)\n",
        "    df_football_stats['PassComp%'] = ((df_football_stats['PassCompleted'] / df_football_stats['Pass']) * 100).round(2)\n",
        "    df_football_stats['Assist'] = ((foot_perf_data['Assists'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['Cross'] = ((foot_perf_data['PasCrs'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['CrossCompleted'] = ((foot_perf_data['CrsPA'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['CrossComp%'] = ((df_football_stats['CrossCompleted'] / df_football_stats['Cross']) * 100).round(\n",
        "        2)\n",
        "    df_football_stats['Tackle_Won'] = ((foot_perf_data['TklWon'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['SucDribble'] = ((foot_perf_data['DriSucc'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['Dribble'] = ((foot_perf_data['DriAtt'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['DribbleComp%'] = ((df_football_stats['SucDribble'] / df_football_stats['Dribble']) * 100).round(\n",
        "        2)\n",
        "    df_football_stats['YCards'] = ((foot_perf_data['CrdY'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['RCards'] = ((foot_perf_data['CrdR'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['Fls'] = ((foot_perf_data['Fls'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['Fld'] = ((foot_perf_data['Fld'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['OGoals'] = ((foot_perf_data['OG'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['AerWon'] = ((foot_perf_data['AerWon'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['AerLost'] = ((foot_perf_data['AerLost'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['AerWon%'] = (\n",
        "                (foot_perf_data['AerWon'] / (foot_perf_data['AerWon'] + foot_perf_data['AerLost'])) * 100).round(2)\n",
        "    df_football_stats = df_football_stats.iloc[:, [0, 1, 2, 3, 4, 5, 6, 11, 12, 14, 15, 19, 22]]\n",
        "    df_football_stats = df_football_stats.fillna(0)\n",
        "\n",
        "    # Create a folder named \"Formatted_result\" if it doesn't exist\n",
        "    folder_path = \"Formatted_result\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Save the DataFrame as a CSV file inside the \"Formatted_result\" folder\n",
        "    file_path = os.path.join(folder_path, \"formatted_football_stats.csv\")\n",
        "    df_football_stats.to_csv(file_path, index=False)\n",
        "\n",
        "\n",
        "    # Print the new Dataframe Result\n",
        "    print(df_football_stats)\n",
        "    cleaned_foot_perf_data = df_football_stats.to_json(orient='records')\n",
        "    return cleaned_foot_perf_data\n",
        "\n",
        "#cleaned_df_perf_data = clean_foot_perf(foot_perf_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def clean_market_value(ti):\n",
        "    print(\"formatted market value data task\")\n",
        "    market_value_data = ti.xcom_pull(task_ids='source_to_raw2')\n",
        "    market_value_data = pd.read_json(market_value_data)\n",
        "\n",
        "    filtered_data = market_value_data.drop(columns=market_value_data.columns[0],\n",
        "                                           axis=1)  # Delete the 1st column which was \"Unnamed\"\n",
        "\n",
        "    filtered_data['market_int'] = filtered_data['Market value'].str.replace('m', '').str.replace(\"€\",\n",
        "                                                                                                 \"\")  # Convert the column into a Float type\n",
        "    filtered_data['market_int'] = filtered_data['market_int'].astype(float)\n",
        "\n",
        "    filtered_data = filtered_data.drop([\"Nation\", \"Pos\", \"Market value\"], axis=1)  # We delete useless columns\n",
        "\n",
        "    # Create a folder named \"Formatted_result\" if it doesn't exist\n",
        "    folder_path = \"Formatted_result\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Save the DataFrame as a CSV file inside the \"Formatted_result\" folder\n",
        "    file_path = os.path.join(folder_path, \"filtered_market_value.csv\")\n",
        "    filtered_data.to_csv(file_path, index=False)\n",
        "    print(filtered_data)\n",
        "    clean_market_value_data = filtered_data.to_json(orient='records')\n",
        "\n",
        "\n",
        "    return clean_market_value_data\n",
        "\n",
        "#cleaned_df_value_data = clean_market_value(market_value_data)\n",
        "\n",
        "\n",
        "\n",
        "def join_datasets(ti):\n",
        "    # We load our 2 formatted Datasets\n",
        "    print(\"join 2 datasets task\")\n",
        "    cleaned_df_perf = ti.xcom_pull(task_ids='raw_to_formatted1')\n",
        "    cleaned_df_perf = pd.read_json(cleaned_df_perf)\n",
        "    cleaned_df_value = ti.xcom_pull(task_ids='raw_to_formatted2')\n",
        "    cleaned_df_value = pd.read_json(cleaned_df_value)\n",
        "    print(cleaned_df_value)\n",
        "    print(cleaned_df_perf)\n",
        "\n",
        "    # After loading them, we join foot_perf_data and market_value_data into a single df : merged_df\n",
        "    merged_df = pd.merge(cleaned_df_perf, cleaned_df_value, on='Player', how='left')\n",
        "    median_value = merged_df['market_int'].median()\n",
        "    merged_df['market_int'] = merged_df['market_int'].fillna(round(median_value / 2, 2))\n",
        "    merged_df['Players Age interval'] = merged_df['Age'].apply(interval_age)\n",
        "    merged_df['Match played interval'] = merged_df['MP'].apply(interval_match)\n",
        "    merged_df['Goals interval'] = merged_df['Goals'].apply(interval_goal)\n",
        "    merged_df['Successful Pass rate (%)'] = merged_df['PassComp%'].apply(interval_pass)\n",
        "    merged_df['Successful Dribble rate (%)'] = merged_df['DribbleComp%'].apply(interval_dribble)\n",
        "    merged_df['market_interval (M €)'] = merged_df['market_int'].apply(interval_value)\n",
        "    merged_df = merged_df.rename(columns={'market_int': 'MarketValue (M €)'})\n",
        "    merged_df = merged_df.drop_duplicates([\"Player\"], keep=\"first\")\n",
        "\n",
        "    # Create a folder named \"Combined_result\" if it doesn't exist\n",
        "    folder_path = \"Combined_result\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Save the DataFrame as a CSV file inside the \"Combined_result\" folder\n",
        "    file_path = os.path.join(folder_path, \"merged_data.csv\")\n",
        "    merged_df.to_csv(file_path, index=False)\n",
        "    print(merged_df)\n",
        "\n",
        "    merged_data = merged_df.to_json(orient='records')\n",
        "    return merged_data\n",
        "\n",
        "#merged_data = join_datasets(ti)\n",
        "\n",
        "\n",
        "\n",
        "def nationalteam_selection(ti):\n",
        "    players_data = ti.xcom_pull(task_ids='combine')\n",
        "    players_data = pd.read_json(players_data)\n",
        "    players_by_nation_data = filtering_players_by_nation(players_data)\n",
        "    print(players_by_nation_data.head(34))\n",
        "\n",
        "    folder_path = \"International_Team_Selection\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Save the DataFrame as a CSV file inside the \"International_Team_Selection\" folder\n",
        "    file_path = os.path.join(folder_path, \"players_by_nation_data.csv\")\n",
        "    players_by_nation_data.to_csv(file_path, index=False)\n",
        "\n",
        "    players_by_nation_data = players_by_nation_data.to_json(orient='records')\n",
        "    return players_by_nation_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def index_to_elastic(ti):\n",
        "    final_data_save = ti.xcom_pull(task_ids='players_nation')\n",
        "    final_data_elastic = pd.read_json(final_data_save)\n",
        "    es_client = connections.create_connection(hosts=['http://localhost:9200/'])\n",
        "    helpers.bulk(es_client, indexing_function(final_data_elastic))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "source_to_raw1 = PythonOperator(\n",
        "    task_id='source_to_raw1',\n",
        "    python_callable=load_foot_perf,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "source_to_raw2 = PythonOperator(\n",
        "    task_id='source_to_raw2',\n",
        "    python_callable=load_market_value,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "raw_to_formatted1 = PythonOperator(\n",
        "    task_id='raw_to_formatted1',\n",
        "    python_callable=clean_foot_perf,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "raw_to_formatted2 = PythonOperator(\n",
        "    task_id='raw_to_formatted2',\n",
        "    python_callable=clean_market_value,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "combine = PythonOperator(\n",
        "    task_id='combine',\n",
        "    python_callable=join_datasets,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "filter_players_nation = PythonOperator(\n",
        "    task_id='players_nation',\n",
        "    python_callable=nationalteam_selection,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "index_to_elastic = PythonOperator(\n",
        "    task_id='index_to_elastic',\n",
        "    python_callable=index_to_elastic,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "source_to_raw1 >> raw_to_formatted1\n",
        "source_to_raw2 >> raw_to_formatted2\n",
        "[raw_to_formatted1, raw_to_formatted2] >> combine >> filter_players_nation >> index_to_elastic\n"
      ],
      "metadata": {
        "id": "N3jiJDh75Nx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "index_elastic.py"
      ],
      "metadata": {
        "id": "u698ngLB5T1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Elasticsearch instance\n",
        "\n",
        "\n",
        "def indexing_function(df):\n",
        "    # Index the data in Elasticsearch\n",
        "    df_iter = df.iterrows()\n",
        "    for index, document in df_iter:\n",
        "        yield {\n",
        "            \"_index\": 'test10juin',\n",
        "            \"_type\": \"_doc\",\n",
        "            \"_source\": document,\n",
        "        }"
      ],
      "metadata": {
        "id": "IoWW16yq5WzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "formatting_functions.py"
      ],
      "metadata": {
        "id": "b838AFxb5Zhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create all useful functions which aims to help to manipulate our data\n",
        "\n",
        "def interval_age(age_int):\n",
        "    if (age_int < 18):\n",
        "        return \"[- de 18]\"  # catégorie de moins de 15 M €\n",
        "    elif (age_int > 18) & (age_int < 22):\n",
        "        return \"[18, 22[\"\n",
        "    elif (age_int >= 22) & (age_int < 24):\n",
        "        return \"[22, 24[\"\n",
        "    elif (age_int >= 24) & (age_int < 25):\n",
        "        return \"[24, 25[\"\n",
        "    elif (age_int >= 25) & (age_int < 28):\n",
        "        return \"[25, 28[\"\n",
        "    elif (age_int >= 28) & (age_int < 30):\n",
        "        return \"[28, 30[\"\n",
        "    elif (age_int >= 30) & (age_int < 35):\n",
        "        return \"[30, 35[\"\n",
        "    elif (age_int >= 35):\n",
        "        return \"[35 ou +]\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# à ajouter dans un autre fichier python\n",
        "def interval_match(match_int):\n",
        "    if (match_int < 1):\n",
        "        return \"[- de 1]\"  # catégorie de moins de 15 M €\n",
        "    elif (match_int >= 1) & (match_int < 5):\n",
        "        return \"[1, 5[\"\n",
        "    elif (match_int >= 5) & (match_int < 10):\n",
        "        return \"[5, 10[\"\n",
        "    elif (match_int >= 10) & (match_int < 15):\n",
        "        return \"[10, 15[\"\n",
        "    elif (match_int >= 15) & (match_int < 20):\n",
        "        return \"[15, 20[\"\n",
        "    elif (match_int >= 20) & (match_int < 25):\n",
        "        return \"[20, 25[\"\n",
        "    elif (match_int >= 25):\n",
        "        return \"[25 ou +]\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# à ajouter dans un autre fichier python\n",
        "def interval_goal(match_int):\n",
        "    if (match_int < 10):\n",
        "        return \"[- de 10]\"  # catégorie de moins de 15 M €\n",
        "    elif (match_int > 10) & (match_int < 20):\n",
        "        return \"[10, 20[\"\n",
        "    elif (match_int >= 20) & (match_int < 25):\n",
        "        return \"[20, 25[\"\n",
        "    elif (match_int >= 25) & (match_int < 30):\n",
        "        return \"[25, 30[\"\n",
        "    elif (match_int >= 30) & (match_int < 35):\n",
        "        return \"[30, 35[\"\n",
        "    elif (match_int >= 35):\n",
        "        return \"[35 ou +]\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# à ajouter dans un autre fichier python\n",
        "def interval_pass(PassComp):\n",
        "    if (PassComp < 15):\n",
        "        return \"[- de 15]\"  # catégorie de moins de 15 M €\n",
        "    elif (PassComp > 15) & (PassComp < 30):\n",
        "        return \"[15, 30[\"\n",
        "    elif (PassComp >= 30) & (PassComp < 50):\n",
        "        return \"[30, 50[\"\n",
        "    elif (PassComp >= 50) & (PassComp < 60):\n",
        "        return \"[50, 60[\"\n",
        "    elif (PassComp >= 60) & (PassComp < 80):\n",
        "        return \"[60, 80[\"\n",
        "    elif (PassComp >= 80) & (PassComp < 90):\n",
        "        return \"[80, 90[\"\n",
        "    elif (PassComp >= 90) & (PassComp < 100):\n",
        "        return \"[90, 100]\"\n",
        "    elif (PassComp == 100):\n",
        "        return \"[100]\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# à ajouter dans un autre fichier python\n",
        "def interval_dribble(DribbleComp):\n",
        "    if (DribbleComp < 15):\n",
        "        return \"[- de 15]\"  # catégorie de moins de 15 M €\n",
        "    elif (DribbleComp > 15) & (DribbleComp < 30):\n",
        "        return \"[15, 30[\"\n",
        "    elif (DribbleComp >= 30) & (DribbleComp < 50):\n",
        "        return \"[30, 50[\"\n",
        "    elif (DribbleComp >= 50) & (DribbleComp < 60):\n",
        "        return \"[50, 60[\"\n",
        "    elif (DribbleComp >= 60) & (DribbleComp < 80):\n",
        "        return \"[60, 80[\"\n",
        "    elif (DribbleComp >= 80) & (DribbleComp < 90):\n",
        "        return \"[80, 90[\"\n",
        "    elif (DribbleComp >= 90) & (DribbleComp < 100):\n",
        "        return \"[90, 100]\"\n",
        "    elif (DribbleComp == 100):\n",
        "        return \"[100]\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# à ajouter dans un autre fichier python\n",
        "def interval_value(market_int):\n",
        "    if (market_int < 15):\n",
        "        return \"[- de 15]\"  # catégorie de moins de 15 M €\n",
        "    elif (market_int > 15) & (market_int < 20):\n",
        "        return \"[15, 20[\"\n",
        "    elif (market_int >= 20) & (market_int < 30):\n",
        "        return \"[20, 30[\"\n",
        "    elif (market_int >= 30) & (market_int < 40):\n",
        "        return \"[30, 40[\"\n",
        "    elif (market_int >= 40) & (market_int < 50):\n",
        "        return \"[40, 50[\"\n",
        "    elif (market_int >= 50) & (market_int < 60):\n",
        "        return \"[50, 60[\"\n",
        "    elif (market_int >= 60):\n",
        "        return \"[60 ou +]\"\n",
        "    else:\n",
        "        return \"\"\n"
      ],
      "metadata": {
        "id": "UYB72vjo5ZWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "team_by_nation.py"
      ],
      "metadata": {
        "id": "JOYS8c8a5f5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def filtering_players_by_nation(merged_data):\n",
        "    print(\"merged data task\")\n",
        "    df_football_stat = merged_data.sort_values(by='MarketValue (M €)', ascending=False)\n",
        "    filtered_df = df_football_stat.groupby(\"Nation\").filter(lambda x: len(x) >= 11).groupby(\"Nation\").apply(\n",
        "        lambda x: x[\n",
        "            (\n",
        "                (x['Goals'] >= 10) |\n",
        "                (x['Assist'] >= 10) |\n",
        "                (x['MP'] >= 20) |\n",
        "                (x['Tackle_Won'] >= 20) |\n",
        "                (x['PassComp%'] >= 60) |\n",
        "                (x['DribbleComp%'] >= 60) |\n",
        "                (x['MarketValue (M €)'] >= 15)\n",
        "            ) & (x['Pos'] != \"GK\")\n",
        "        ].head(10)\n",
        "    ).reset_index(drop=True)\n",
        "    df_football_stat = merged_data.sort_values(by='MarketValue (M €)', ascending=False)\n",
        "    gk_df = df_football_stat.groupby(\"Nation\").filter(lambda x: len(x) >= 11).groupby(\"Nation\").apply(\n",
        "        lambda x: x[\n",
        "            (\n",
        "                (x['Pos'] == 'GK') &\n",
        "                (x['MP'] >= 10) |\n",
        "                (x['Pos'] == 'GK') &\n",
        "                (x['MarketValue (M €)'] >= 15)\n",
        "            )\n",
        "        ].head(1)\n",
        "    )\n",
        "    filtered_df = pd.concat([filtered_df, gk_df]).reset_index(drop=True)\n",
        "    filtered_df = filtered_df.sort_values(by='Nation')  # Sort by 'Nation' column\n",
        "    filtered_df.reset_index(drop=True, inplace=True)  # Reset the index\n",
        "\n",
        "    # Create a folder named \"Final_file_to_index\" if it doesn't exist\n",
        "    folder_path = \"Final_file_to_index\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Save the DataFrame as a CSV file inside the \"Final_file_to_index\" folder\n",
        "    file_path = os.path.join(folder_path, \"equipe_nation.csv\")\n",
        "    filtered_df.to_csv(file_path, index=False)\n",
        "\n",
        "    return filtered_df\n"
      ],
      "metadata": {
        "id": "qz8gT81j5gU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert python -> PySpark"
      ],
      "metadata": {
        "id": "lDz1cCppwMJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_foot_perf(ti):\n",
        "    print(\"cleaned foot perf data task\")\n",
        "    foot_perf_data = ti.xcom_pull(task_ids='source_to_raw1')\n",
        "    foot_perf_data = pd.read_json(foot_perf_data)\n",
        "    # Formatting our data by filtering on a new dataframe based on the original one\n",
        "    df_football_stats = pd.DataFrame()\n",
        "    df_football_stats['Player'] = foot_perf_data['Player']\n",
        "    df_football_stats['Nation'] = foot_perf_data['Nation']\n",
        "    df_football_stats['Pos'] = foot_perf_data['Pos']\n",
        "    df_football_stats['Squad'] = foot_perf_data['Squad']\n",
        "    df_football_stats['Comp'] = foot_perf_data['Comp']\n",
        "    df_football_stats['Age'] = foot_perf_data['Age']\n",
        "    df_football_stats['MP'] = foot_perf_data['MP']\n",
        "    df_football_stats['G/90'] = foot_perf_data['Goals']\n",
        "    df_football_stats['G/Sh'] = foot_perf_data['G/Sh']\n",
        "    df_football_stats['PKGoals'] = ((foot_perf_data['ShoPK'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['PKAttempted'] = ((foot_perf_data['PKatt'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['Goals'] = ((foot_perf_data['Goals'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['Pass'] = ((foot_perf_data['PasTotAtt'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['PassCompleted'] = ((foot_perf_data['PasTotCmp'] * foot_perf_data['Min']) / 90).round(0).astype(\n",
        "        int)\n",
        "    df_football_stats['PassComp%'] = ((df_football_stats['PassCompleted'] / df_football_stats['Pass']) * 100).round(2)\n",
        "    df_football_stats['Assist'] = ((foot_perf_data['Assists'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['Cross'] = ((foot_perf_data['PasCrs'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['CrossCompleted'] = ((foot_perf_data['CrsPA'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['CrossComp%'] = ((df_football_stats['CrossCompleted'] / df_football_stats['Cross']) * 100).round(\n",
        "        2)\n",
        "    df_football_stats['Tackle_Won'] = ((foot_perf_data['TklWon'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['SucDribble'] = ((foot_perf_data['DriSucc'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['Dribble'] = ((foot_perf_data['DriAtt'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['DribbleComp%'] = ((df_football_stats['SucDribble'] / df_football_stats['Dribble']) * 100).round(\n",
        "        2)\n",
        "    df_football_stats['YCards'] = ((foot_perf_data['CrdY'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['RCards'] = ((foot_perf_data['CrdR'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['Fls'] = ((foot_perf_data['Fls'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['Fld'] = ((foot_perf_data['Fld'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['OGoals'] = ((foot_perf_data['OG'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['AerWon'] = ((foot_perf_data['AerWon'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['AerLost'] = ((foot_perf_data['AerLost'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['AerWon%'] = (\n",
        "                (foot_perf_data['AerWon'] / (foot_perf_data['AerWon'] + foot_perf_data['AerLost'])) * 100).round(2)\n",
        "    df_football_stats = df_football_stats.iloc[:, [0, 1, 2, 3, 4, 5, 6, 11, 12, 14, 15, 19, 22]]\n",
        "    df_football_stats = df_football_stats.fillna(0)\n",
        "\n",
        "    # Create a folder named \"Formatted_result\" if it doesn't exist\n",
        "    folder_path = \"Formatted_result\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Save the DataFrame as a CSV file inside the \"Formatted_result\" folder\n",
        "    file_path = os.path.join(folder_path, \"formatted_football_stats.csv\")\n",
        "    df_football_stats.to_csv(file_path, index=False)\n",
        "\n",
        "\n",
        "    # Print the new Dataframe Result\n",
        "    print(df_football_stats)\n",
        "    cleaned_foot_perf_data = df_football_stats.to_json(orient='records')\n",
        "    return cleaned_foot_perf_data"
      ],
      "metadata": {
        "id": "_aZ_wLMZwLcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------"
      ],
      "metadata": {
        "id": "QM4PZScb147s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Méthode en Python (pandas)"
      ],
      "metadata": {
        "id": "2IzHBSzDLM_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_foot_perf(ti):\n",
        "    print(\"cleaned foot perf data task\")\n",
        "    foot_perf_data = ti.xcom_pull(task_ids='source_to_raw1')\n",
        "    foot_perf_data = pd.read_json(foot_perf_data)\n",
        "    # Formatting our data by filtering on a new dataframe based on the original one\n",
        "    df_football_stats = pd.DataFrame()\n",
        "    df_football_stats['Player'] = foot_perf_data['Player']\n",
        "    df_football_stats['Nation'] = foot_perf_data['Nation']\n",
        "    df_football_stats['Pos'] = foot_perf_data['Pos']\n",
        "    df_football_stats['Squad'] = foot_perf_data['Squad']\n",
        "    df_football_stats['Comp'] = foot_perf_data['Comp']\n",
        "    df_football_stats['Age'] = foot_perf_data['Age']\n",
        "    df_football_stats['MP'] = foot_perf_data['MP']\n",
        "\n",
        "    df_football_stats['G/90'] = foot_perf_data['Goals']\n",
        "    df_football_stats['G/Sh'] = foot_perf_data['G/Sh']\n",
        "    df_football_stats['Goals'] = ((foot_perf_data['Goals'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "\n",
        "    df_football_stats['Pass'] = ((foot_perf_data['PasTotAtt'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['PassCompleted'] = ((foot_perf_data['PasTotCmp'] * foot_perf_data['Min']) / 90).round(0).astype(\n",
        "        int)\n",
        "    df_football_stats['PassComp%'] = ((df_football_stats['PassCompleted'] / df_football_stats['Pass']) * 100).round(2)\n",
        "    df_football_stats['Assist'] = ((foot_perf_data['Assists'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "\n",
        "    df_football_stats['Tackle_Won'] = ((foot_perf_data['TklWon'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "\n",
        "    df_football_stats['SucDribble'] = ((foot_perf_data['DriSucc'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['Dribble'] = ((foot_perf_data['DriAtt'] * foot_perf_data['Min']) / 90).round(0).astype(int)\n",
        "    df_football_stats['DribbleComp%'] = ((df_football_stats['SucDribble'] / df_football_stats['Dribble']) * 100).round(\n",
        "        2)\n",
        "    df_football_stats = df_football_stats.iloc[:, [0, 1, 2, 3, 4, 5, 6, 11]]\n",
        "    df_football_stats = df_football_stats.fillna(0)\n",
        "\n",
        "    # Create a folder named \"Formatted_result\" if it doesn't exist\n",
        "    folder_path = \"Formatted_result\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Save the DataFrame as a CSV file inside the \"Formatted_result\" folder\n",
        "    file_path = os.path.join(folder_path, \"formatted_football_stats.csv\")\n",
        "    df_football_stats.to_csv(file_path, index=False)\n",
        "\n",
        "\n",
        "    # Print the new Dataframe Result\n",
        "    print(df_football_stats)\n",
        "    cleaned_foot_perf_data = df_football_stats.to_json(orient='records')\n",
        "    return cleaned_foot_perf_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def clean_market_value(ti):\n",
        "    print(\"formatted market value data task\")\n",
        "    market_value_data = ti.xcom_pull(task_ids='source_to_raw2')\n",
        "    market_value_data = pd.read_json(market_value_data)\n",
        "\n",
        "    filtered_data = market_value_data.drop(columns=market_value_data.columns[0],\n",
        "                                           axis=1)  # Delete the 1st column which was \"Unnamed\"\n",
        "\n",
        "    filtered_data['market_int'] = filtered_data['Market value'].str.replace('m', '').str.replace(\"€\",\n",
        "                                                                                                 \"\")  # Convert the column into a Float type\n",
        "    filtered_data['market_int'] = filtered_data['market_int'].astype(float)\n",
        "\n",
        "    filtered_data = filtered_data.drop([\"Nation\", \"Pos\", \"Market value\"], axis=1)  # We delete useless columns\n",
        "\n",
        "    # Create a folder named \"Formatted_result\" if it doesn't exist\n",
        "    folder_path = \"Formatted_result\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Save the DataFrame as a CSV file inside the \"Formatted_result\" folder\n",
        "    file_path = os.path.join(folder_path, \"filtered_market_value.csv\")\n",
        "    filtered_data.to_csv(file_path, index=False)\n",
        "    print(filtered_data)\n",
        "    clean_market_value_data = filtered_data.to_json(orient='records')\n",
        "\n",
        "\n",
        "    return clean_market_value_data\n",
        "\n",
        "#cleaned_df_value_data = clean_market_value(market_value_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def join_datasets(ti):\n",
        "    # We load our 2 formatted Datasets\n",
        "    print(\"join 2 datasets task\")\n",
        "    cleaned_df_perf = ti.xcom_pull(task_ids='raw_to_formatted1')\n",
        "    cleaned_df_perf = pd.read_json(cleaned_df_perf)\n",
        "    cleaned_df_value = ti.xcom_pull(task_ids='raw_to_formatted2')\n",
        "    cleaned_df_value = pd.read_json(cleaned_df_value)\n",
        "    print(cleaned_df_value)\n",
        "    print(cleaned_df_perf)\n",
        "\n",
        "    # After loading them, we join foot_perf_data and market_value_data into a single df : merged_df\n",
        "    merged_df = pd.merge(cleaned_df_perf, cleaned_df_value, on='Player', how='left')\n",
        "    median_value = merged_df['market_int'].median()\n",
        "    merged_df['market_int'] = merged_df['market_int'].fillna(round(median_value / 2, 2))\n",
        "    merged_df['Players Age interval'] = merged_df['Age'].apply(interval_age)\n",
        "    merged_df['Match played interval'] = merged_df['MP'].apply(interval_match)\n",
        "    merged_df['Goals interval'] = merged_df['Goals'].apply(interval_goal)\n",
        "    merged_df['Successful Pass rate (%)'] = merged_df['PassComp%'].apply(interval_pass)\n",
        "    merged_df['Successful Dribble rate (%)'] = merged_df['DribbleComp%'].apply(interval_dribble)\n",
        "    merged_df['market_interval (M €)'] = merged_df['market_int'].apply(interval_value)\n",
        "    merged_df = merged_df.rename(columns={'market_int': 'MarketValue (M €)'})\n",
        "    merged_df = merged_df.drop_duplicates([\"Player\"], keep=\"first\")\n",
        "\n",
        "    # Create a folder named \"Combined_result\" if it doesn't exist\n",
        "    folder_path = \"Combined_result\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Save the DataFrame as a CSV file inside the \"Combined_result\" folder\n",
        "    file_path = os.path.join(folder_path, \"merged_data.csv\")\n",
        "    merged_df.to_csv(file_path, index=False)\n",
        "    print(merged_df)\n",
        "\n",
        "    merged_data = merged_df.to_json(orient='records')\n",
        "    return merged_data\n",
        "\n",
        "#merged_data = join_datasets(ti)\n"
      ],
      "metadata": {
        "id": "Cw-jG1nzzwy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------"
      ],
      "metadata": {
        "id": "9KXvbAy3188-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Méthode PySpark"
      ],
      "metadata": {
        "id": "sztrvrsy17SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "ihqENfq82MTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import regexp_replace, col, round\n",
        "\n",
        "def clean_foot_perf(df):\n",
        "    print(\"cleaned foot perf data task\")\n",
        "    foot_perf_data = df\n",
        "    \n",
        "    # Create SparkSession if it doesn't exist\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "    # Convert Pandas DataFrame to PySpark DataFrame\n",
        "    df_foot_perf_data = spark.createDataFrame(foot_perf_data)\n",
        "\n",
        "    print(df_foot_perf_data)\n",
        "\n",
        "    # Generate the schema based on DataFrame columns\n",
        "    schema = df_foot_perf_data.schema\n",
        "\n",
        "    # Apply the schema to the DataFrame\n",
        "    df_foot_perf_data = spark.createDataFrame(foot_perf_data, schema)\n",
        "\n",
        "    # Continue with the remaining code...\n",
        "\n",
        "    # Print the new DataFrame Result\n",
        "    df_foot_perf_data.show()\n",
        "\n",
        "    # Add the additional edits\n",
        "    df_football_stats = df_foot_perf_data.withColumn('Player', df_foot_perf_data['Player']) \\\n",
        "        .withColumn('Nation', df_foot_perf_data['Nation']) \\\n",
        "        .withColumn('Pos', df_foot_perf_data['Pos']) \\\n",
        "        .withColumn('Squad', df_foot_perf_data['Squad']) \\\n",
        "        .withColumn('Comp', df_foot_perf_data['Comp']) \\\n",
        "        .withColumn('Age', df_foot_perf_data['Age']) \\\n",
        "        .withColumn('MP', df_foot_perf_data['MP']) \\\n",
        "        .withColumn('G/90', df_foot_perf_data['Goals']) \\\n",
        "        .withColumn('G/Sh', df_foot_perf_data['G/Sh']) \\\n",
        "        .withColumn('Goals', round((df_foot_perf_data['Goals'] * df_foot_perf_data['Min']) / 90).cast('integer')) \\\n",
        "        .withColumn('Pass', round((df_foot_perf_data['PasTotAtt'] * df_foot_perf_data['Min']) / 90).cast('integer')) \\\n",
        "        .withColumn('PassCompleted', round((df_foot_perf_data['PasTotCmp'] * df_foot_perf_data['Min']) / 90).cast('integer')) \\\n",
        "        .withColumn('PassComp%', round((df_foot_perf_data['PasTotCmp'] / df_foot_perf_data['PasTotAtt']) * 100, 2)) \\\n",
        "        .withColumn('Assist', round((df_foot_perf_data['Assists'] * df_foot_perf_data['Min']) / 90).cast('integer')) \\\n",
        "        .withColumn('Tackle_Won', round((df_foot_perf_data['TklWon'] * df_foot_perf_data['Min']) / 90).cast('integer')) \\\n",
        "        .withColumn('SucDribble', round((df_foot_perf_data['DriSucc'] * df_foot_perf_data['Min']) / 90).cast('integer')) \\\n",
        "        .withColumn('Dribble', round((df_foot_perf_data['DriAtt'] * df_foot_perf_data['Min']) / 90).cast('integer')) \\\n",
        "        .withColumn('DribbleComp%', round((df_foot_perf_data['DriSucc'] / df_foot_perf_data['DriAtt']) * 100, 2)) \\\n",
        "\n",
        "    # Select the desired columns\n",
        "    df_football_stats = df_football_stats.select('Player', 'Nation', 'Pos', 'Squad', 'Comp', 'Age', 'MP', 'G/90', 'G/Sh', 'Goals', 'Pass', 'PassCompleted',\n",
        "                                                 'PassComp%', 'Assist',\n",
        "                                                 'Tackle_Won', 'SucDribble', 'Dribble', 'DribbleComp%')\n",
        "    # Fill missing values with 0\n",
        "    df_football_stats = df_football_stats.na.fill(0)\n",
        "\n",
        "    # Save the DataFrame as a CSV file inside the \"Formatted_result\" folder\n",
        "    folder_path = \"Formatted_result\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Save the DataFrame as a CSV file inside the \"Formatted_result\" folder\n",
        "    file_path = os.path.join(folder_path, \"formatted_football_stats.csv\")\n",
        "    df_football_stats.toPandas().to_csv(file_path, index=False)\n",
        "\n",
        "    # Print the new DataFrame Result\n",
        "    df_football_stats.show(truncate=False)\n",
        "\n",
        "    # Convert the DataFrame to JSON format\n",
        "    #cleaned_foot_perf_data = df_football_stats.toJSON().collect()\n",
        "    cleaned_foot_perf_data = df_football_stats.toPandas()\n",
        "    cleaned_foot_perf_data = cleaned_foot_perf_data.to_json(orient='records')\n",
        "\n",
        "    return cleaned_foot_perf_data\n"
      ],
      "metadata": {
        "id": "Uo4Foplo197w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perf = clean_foot_perf(load_foot_perf())\n",
        "perf"
      ],
      "metadata": {
        "id": "ieXqZqzj1_01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "def clean_market_value(df):\n",
        "    print(\"formatted market value data task\")\n",
        "\n",
        "    # Create SparkSession if it doesn't exist\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "    # Convert Pandas DataFrame to PySpark DataFrame\n",
        "    df_market_value_data = spark.createDataFrame(df)\n",
        "\n",
        "    # Drop the first column\n",
        "    df_market_value_data = df_market_value_data.drop(df_market_value_data.columns[0])\n",
        "\n",
        "    # Apply transformations to the columns\n",
        "    df_market_value_data = df_market_value_data.withColumn(\"market_int\", regexp_replace(col(\"Market value\"), \"m|€\", \"\").cast(FloatType()))\n",
        "    df_market_value_data = df_market_value_data.drop(\"Nation\", \"Pos\", \"Market value\")\n",
        "\n",
        "    df_market_value_data.show(truncate=False)\n",
        "\n",
        "    # Create a folder named \"Formatted_result\" if it doesn't exist\n",
        "    folder_path = \"Formatted_result\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Save the DataFrame as a CSV file inside the \"Formatted_result\" folder\n",
        "    file_path = os.path.join(folder_path, \"filtered_market_value.csv\")\n",
        "    df_market_value_data.toPandas().to_csv(file_path, index=False)\n",
        "\n",
        "    # Convert the DataFrame to JSON format\n",
        "    #clean_market_value_data = df_market_value_data.toJSON().collect()\n",
        "    df_market_value_data = df_market_value_data.toPandas()\n",
        "    clean_market_value_data = df_market_value_data.to_json(orient='records')\n",
        "\n",
        "    return clean_market_value_data\n"
      ],
      "metadata": {
        "id": "rDTdqH412ANV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value = clean_market_value(load_market_value())\n",
        "value"
      ],
      "metadata": {
        "id": "y8kXypuG2C7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def join_datasets(clean_foot_perf, clean_foot_value):\n",
        "    # Create SparkSession if it doesn't exist\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "    # Create DataFrames from the input data\n",
        "    clean_foot_perf_df = pd.read_json(clean_foot_perf)\n",
        "    clean_foot_value_df = pd.read_json(clean_foot_value)\n",
        "\n",
        "    # Join the DataFrames\n",
        "    merged_df = pd.merge(clean_foot_perf_df, clean_foot_value_df, on='Player', how='left')\n",
        "\n",
        "    median_value = merged_df['market_int'].median()\n",
        "    merged_df['market_int'] = merged_df['market_int'].fillna((median_value/2))\n",
        "    merged_df['Match played interval'] = merged_df['MP'].apply(interval_match)\n",
        "    merged_df['Goals interval'] = merged_df['Goals'].apply(interval_goal)\n",
        "    merged_df['Successful Pass rate (%)'] = merged_df['PassComp%'].apply(interval_pass)\n",
        "    merged_df['Successful Dribble rate (%)'] = merged_df['DribbleComp%'].apply(interval_dribble)\n",
        "    merged_df['market_interval (M €)'] = merged_df['market_int'].apply(interval_value)\n",
        "    merged_df = merged_df.rename(columns={'market_int': 'MarketValue (M €)'})\n",
        "    merged_df = merged_df.drop_duplicates([\"Player\"], keep=\"first\")\n",
        "\n",
        "    # Create a folder named \"Combined_result\" if it doesn't exist\n",
        "    folder_path = \"Combined_result\"\n",
        "    if not os.path.exists(folder_path):\n",
        "      os.makedirs(folder_path)\n",
        "\n",
        "    # Save the DataFrame as a CSV file inside the \"Combined_result\" folder\n",
        "    file_path = os.path.join(folder_path, \"merged_data.csv\")\n",
        "    merged_df.to_csv(file_path, index=False)\n",
        "\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    spark_merged_df = spark.createDataFrame(merged_df)\n",
        "    spark_merged_df.show(truncate=False)\n",
        "\n",
        "\n",
        "    return merged_df\n",
        "\n"
      ],
      "metadata": {
        "id": "horuK1_S2ETV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_datasets(perf, value)"
      ],
      "metadata": {
        "id": "-BM9Xiq_2F5Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}