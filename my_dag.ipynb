{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8XIuc2QfWz5drpAGakGr+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PuchToTalk/Football_market-value/blob/Airflow-dag/my_dag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1ere version"
      ],
      "metadata": {
        "id": "452i6JzmLatB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DieKTJYSdEMT"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "with DAG(\n",
        "       'my_first_dag',\n",
        "       default_args={\n",
        "           'depends_on_past': False,\n",
        "           'email': ['airflow@example.com'],\n",
        "           'email_on_failure': False,\n",
        "           'email_on_retry': False,\n",
        "           'retries': 1,\n",
        "           'retry_delay': timedelta(minutes=5),\n",
        "       },\n",
        "       description='A first DAG',\n",
        "       schedule=None,\n",
        "       start_date=datetime(2023, 1, 1),\n",
        "       catchup=False,\n",
        "       tags=['example'],\n",
        "\n",
        ") as dag:\n",
        "   dag.doc_md = \"\"\"\n",
        "       This is my DAG in airflow for the Big Data Project.\n",
        "       I can write documentation in Markdown here with **bold text** or __bold text__.\n",
        "   \"\"\"\n",
        "\n",
        "   import pandas as pd\n",
        "   import pyarrow as pa\n",
        "   import pyarrow.parquet as pq\n",
        "\n",
        "\n",
        "   def load_foot_perf():\n",
        "       print(\"load foot perf task\")\n",
        "       # Chargement du fichier foot_perf.csv\n",
        "       foot_perf_data = pd.read_csv(\"/Users/paulc/airflow/dags/foot_perf.csv\")\n",
        "       #foot_perf_data = foot_perf_data.to_json(orient='records')\n",
        "\n",
        "       # Convert DataFrame to PyArrow Table\n",
        "       foot_perf_data = pa.Table.from_pandas(foot_perf_data)\n",
        "       output_1 = 'output_foot_perf.parquet'\n",
        "       pq.write_table(foot_perf_data, output_1)\n",
        "\n",
        "       # Read Parquet file back into a DataFrame\n",
        "       parquet_table1 = pq.read_table(output_1)\n",
        "       foot_perf_data = parquet_table1.to_pandas()\n",
        "       foot_perf_data = foot_perf_data.to_json(orient='records')\n",
        "       return foot_perf_data\n",
        "\n",
        "   foot_perf_data = load_foot_perf()\n",
        "   #print(foot_perf_data)\n",
        "\n",
        "\n",
        "\n",
        "   def load_market_value():\n",
        "       print(\"load market value task\")\n",
        "       # Chargement du fichier market_value.csv\n",
        "       market_value_data = pd.read_csv(\"/Users/paulc/airflow/dags/only_mv2.csv\")\n",
        "       #market_value_data = market_value_data.to_json(orient='records')\n",
        "       # Convert DataFrame to PyArrow Table\n",
        "       market_value_data = pa.Table.from_pandas(market_value_data)\n",
        "       output_2 = 'output_foot_value.parquet'\n",
        "       pq.write_table(market_value_data, output_2)\n",
        "\n",
        "       # Read Parquet file back into a DataFrame\n",
        "       parquet_table2 = pq.read_table(output_2)\n",
        "       market_value_data = parquet_table2.to_pandas()\n",
        "       market_value_data = market_value_data.to_json(orient='records')\n",
        "       return market_value_data\n",
        "\n",
        "\n",
        "   market_value_data = load_market_value()\n",
        "   #print(market_value_data)\n",
        "\n",
        "\n",
        "   def clean_foot_perf():\n",
        "       print(\"cleaned foot perf data task\")\n",
        "       # Nettoyage et filtrage des données de foot_perf\n",
        "       foot_perf_data = pd.read_json(load_foot_perf())\n",
        "       # Read Parquet file back into a DataFrame\n",
        "       output_1 = 'output_foot_perf.parquet'\n",
        "       # Read Parquet file back into a DataFrame\n",
        "       parquet_table1 = pq.read_table(output_1)\n",
        "       foot_perf_data = parquet_table1.to_pandas()\n",
        "       return foot_perf_data\n",
        "\n",
        "\n",
        "   print(clean_foot_perf())\n",
        "\n",
        "\n",
        "   # Perform further operations with cleaned_foot_perf_data if needed\n",
        "\n",
        "   def clean_market_value(market_value_data):\n",
        "       print(\"formatted market value data task\")\n",
        "       market_value_data = pd.DataFrame(market_value_data)\n",
        "       filtered_data = market_value_data.drop(columns= filtered_data.columns[0], axis=1)\n",
        "       filtered_data = filtered_data.drop([\"Nation\", \"Pos\"], axis=1)\n",
        "       filtered_data = filtered_data.to_dict(orient='records')\n",
        "       return filtered_data\n",
        "\n",
        "\n",
        "   def join_datasets(df_football_stats, market_value_data):\n",
        "       print(\"join 2 datasets task \")\n",
        "       # Jointure des données de foot_perf et market_value\n",
        "       merged_data = df_football_stats.merge(market_value_data, on='player', how='inner')\n",
        "       return merged_data\n",
        "\n",
        "\n",
        "   def index_to_elastic(merged_data):\n",
        "       print(\"merged data task\")\n",
        "       # Indexation des données dans ElasticSearch\n",
        "       # Code pour l'indexation des données dans ElasticSearch\n",
        "       #pass\n",
        "\n",
        "   source_to_raw1 = PythonOperator(\n",
        "        task_id='source_to_raw1',\n",
        "        python_callable=load_foot_perf,\n",
        "        #provide_context=True,  # Add this line to pass the context to the function\n",
        "        #op_kwargs={'foot_perf_data': load_foot_perf()}  # Initialize foot_perf_data with None\n",
        "    )\n",
        "\n",
        "   source_to_raw2 = PythonOperator(\n",
        "       task_id='source_to_raw2',\n",
        "       python_callable=load_market_value,\n",
        "       #provide_context=True,  # Add this line to pass the context to the function\n",
        "       #op_kwargs={'market_value_data': load_market_value()}  # Initialize foot_perf_data with None\n",
        "   )\n",
        "\n",
        "   raw_to_formatted1 = PythonOperator(\n",
        "       task_id='raw_to_formatted1',\n",
        "       python_callable=clean_foot_perf,\n",
        "       #op_kwargs={'foot_perf_data': '{{ ti.xcom_pull(task_ids=\"source_to_raw1\") }}'}\n",
        "\n",
        "   )\n",
        "\n",
        "   raw_to_formatted2 = PythonOperator(\n",
        "        task_id='raw_to_formatted2',\n",
        "        python_callable=clean_market_value,\n",
        "        #op_kwargs={'market_value_data': '{{ ti.xcom_pull(task_ids=\"source_to_raw2\") }}'}\n",
        "\n",
        "    )\n",
        "\n",
        "   produce_usage = PythonOperator(\n",
        "       task_id='produce_usage',\n",
        "       python_callable=join_datasets,\n",
        "\n",
        "   )\n",
        "\n",
        "   index_to_elastic = PythonOperator(\n",
        "       task_id='index_to_elastic',\n",
        "       python_callable=index_to_elastic,\n",
        "\n",
        "   )\n",
        "\n",
        "   source_to_raw1 >> raw_to_formatted1\n",
        "   source_to_raw2 >> raw_to_formatted2\n",
        "   [raw_to_formatted1, raw_to_formatted2] >> produce_usage >> index_to_elastic\n",
        "\n",
        "\n",
        "   if __name__ == 'main':\n",
        "        df = clean_foot_perf(load_foot_perf())\n",
        "        df1 = clean_market_value(load_market_value())\n",
        "        df2 = join_datasets(df, df1)\n",
        "        index_to_elastic(df2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2eme version : méthode JSON -> Data -> JSON"
      ],
      "metadata": {
        "id": "7K5nAtRPLYGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.models import DagRun\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "default_args = {\n",
        "    'depends_on_past': False,\n",
        "    'email': ['airflow@example.com'],\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "dag = DAG(\n",
        "    'my_first_dag',\n",
        "    default_args=default_args,\n",
        "    description='A first DAG',\n",
        "    schedule=None,\n",
        "    start_date=datetime(2023, 1, 1),\n",
        "    catchup=False,\n",
        "    tags=['example'],\n",
        ")\n",
        "\n",
        "def load_foot_perf():\n",
        "    print(\"load foot perf task\")\n",
        "    foot_perf_data = pd.read_csv(\"/Users/paulc/airflow/dags/foot_perf.csv\")\n",
        "    print(foot_perf_data)\n",
        "    foot_perf_data_parquet = pa.Table.from_pandas(foot_perf_data)\n",
        "    output_1 = 'output_foot_perf.parquet'\n",
        "    pq.write_table(foot_perf_data_parquet, output_1)\n",
        "    foot_perf_data = foot_perf_data.to_json(orient='records')\n",
        "    print(foot_perf_data)\n",
        "    return foot_perf_datax\n",
        "\n",
        "def load_market_value():\n",
        "    print(\"load market value task\")\n",
        "    market_value_data = pd.read_csv(\"/Users/paulc/airflow/dags/only_mv2.csv\")\n",
        "    market_value_data_parquet = pa.Table.from_pandas(market_value_data)\n",
        "    output_2 = 'output_foot_value.parquet'\n",
        "    pq.write_table(market_value_data_parquet, output_2)\n",
        "    market_value_data = market_value_data.to_json(orient='records')\n",
        "    return market_value_data\n",
        "\n",
        "def clean_foot_perf(ti):\n",
        "    print(\"cleaned foot perf data task\")\n",
        "    foot_perf_data = ti.xcom_pull(task_ids='source_to_raw1')\n",
        "    # Perform cleaning operations on foot_perf_data DataFrame\n",
        "    cleaned_df_perf = pd.read_json(foot_perf_data)\n",
        "    print(cleaned_df_perf)\n",
        "    cleaned_df_perf_data = cleaned_df_perf.to_json(orient='records')\n",
        "    return cleaned_df_perf_data\n",
        "\n",
        "\n",
        "def clean_market_value(ti):\n",
        "    print(\"formatted market value data task\")\n",
        "    market_value_data = ti.xcom_pull(task_ids='source_to_raw2')\n",
        "    # Perform cleaning operations on market_value_data DataFrame\n",
        "    cleaned_df_value = pd.read_json(market_value_data)\n",
        "    print(cleaned_df_value)\n",
        "    cleaned_df_value_data = cleaned_df_value.to_json(orient='records')\n",
        "    return cleaned_df_value_data\n",
        "\n",
        "\n",
        "def join_datasets(ti):\n",
        "    print(\"join 2 datasets task\")\n",
        "    foot_perf_data = ti.xcom_pull(task_ids='raw_to_formatted1')\n",
        "    market_value_data = ti.xcom_pull(task_ids='raw_to_formatted2')\n",
        "    # Join foot_perf_data and market_value_data\n",
        "    merged_data = foot_perf_data.merge(market_value_data, on='player', how='inner')\n",
        "    return merged_data\n",
        "\n",
        "def index_to_elastic(ti):\n",
        "    print(\"index to ElasticSearch task\")\n",
        "    merged_data = ti.xcom_pull(task_ids='combine')\n",
        "    # Index merged_data to ElasticSearch\n",
        "    # ...\n",
        "\n",
        "source_to_raw1 = PythonOperator(\n",
        "    task_id='source_to_raw1',\n",
        "    python_callable=load_foot_perf,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "source_to_raw2 = PythonOperator(\n",
        "    task_id='source_to_raw2',\n",
        "    python_callable=load_market_value,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "raw_to_formatted1 = PythonOperator(\n",
        "    task_id='raw_to_formatted1',\n",
        "    python_callable=clean_foot_perf,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "raw_to_formatted2 = PythonOperator(\n",
        "    task_id='raw_to_formatted2',\n",
        "    python_callable=clean_market_value,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "combine = PythonOperator(\n",
        "    task_id='combine',\n",
        "    python_callable=join_datasets,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "index_to_elastic = PythonOperator(\n",
        "    task_id='index_to_elastic',\n",
        "    python_callable=index_to_elastic,\n",
        "\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "source_to_raw1 >> raw_to_formatted1\n",
        "source_to_raw2 >> raw_to_formatted2\n",
        "[raw_to_formatted1, raw_to_formatted2] >> combine >> index_to_elastic\n"
      ],
      "metadata": {
        "id": "8531lL_GLZDv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}